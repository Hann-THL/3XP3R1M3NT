{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Install Confluent Apache Kafka?\n",
    "- https://nijanthanravi.wordpress.com/2019/07/14/setup-confluent-kafka-on-windows/\n",
    "- https://medium.com/@praveenkumarsingh/confluent-kafka-on-windows-how-to-fix-classpath-is-empty-cf7c31d9c787\n",
    "- https://github.com/confluentinc/confluent-kafka-python\n",
    "\n",
    "### Python Dependencies:\n",
    "- pip install confluent-kafka\n",
    "- pip install avro-python3\n",
    "\n",
    "### MySQL Script:\n",
    "```\n",
    "CREATE TABLE `testdb`.`test_table` (\n",
    "  `int_field` INT NOT NULL AUTO_INCREMENT,\n",
    "  `str_field` VARCHAR(45) NOT NULL,\n",
    "  `double_field` DOUBLE NULL,\n",
    "  `decimal_field` DECIMAL(6,2) NULL,\n",
    "  `date_field` DATE NULL,\n",
    "  `datetime_field` DATETIME NULL,\n",
    "  PRIMARY KEY (`int_field`)\n",
    ");\n",
    "\n",
    "INSERT INTO testdb.test_table (str_field, double_field, decimal_field, date_field, datetime_field)\n",
    "VALUES ('string value', 5.5, 3.25, '2019-02-10', '2019-02-10 12:34:56');\n",
    "\n",
    "UPDATE testdb.test_table\n",
    "SET decimal_field=1234.56,\n",
    "    date_field='2020-01-23',\n",
    "    datetime_field='2020-01-23 01:23:45';\n",
    "\n",
    "DELETE FROM testdb.test_table;\n",
    "```\n",
    "\n",
    "### How to Start Confluent Apache Kafka?\n",
    "1. Open command prompt at confluent folder\n",
    "\n",
    "\n",
    "2. Start Zookeeper<br>\n",
    "   2.1 Execute ***bin\\windows\\zookeeper-server-start.bat etc\\kafka\\zookeeper.properties***<br>\n",
    "   2.2 Default port is ```2181```, change at ***etc\\kafka\\zookeeper.properties*** if necessary<br>\n",
    "   2.3 If encounter ```Classpath``` error:\n",
    "   - Open ***bin\\windows\\kafka-run-class.bat***\n",
    "   - Search for ```rem Classpath addition for core``` in the bat file\n",
    "   - Add following lines above the search line:\n",
    "   ```\n",
    "   rem classpath addition for LSB style path\n",
    "   if exist %BASE_DIR%\\share\\java\\kafka\\* (\n",
    "     call:concat %BASE_DIR%\\share\\java\\kafka\\*\n",
    "   )\n",
    "   ```\n",
    "   - Re-run zookeeper\n",
    "\n",
    "\n",
    "3. Start Kafka Broker<br>\n",
    "   3.1 Execute ***bin\\windows\\kafka-server-start.bat etc\\kafka\\server.properties***<br>\n",
    "   3.2 Delete ***tmp\\kafka-log*** & ***tmp\\zookeeper*** folder if encounter ```ERROR Shutdown broker because all log dirs in tmp\\kafka-logs have failed (kafka.log.LogManager)```\n",
    "\n",
    "\n",
    "4. Start Schema Registry<br>\n",
    "   4.1 Download missing schema-registry files to bin\\windows folder:\n",
    "   - https://github.com/confluentinc/schema-registry/tree/master/bin/windows\n",
    "   4.2 Execute **bin\\windows\\schema-registry-start.bat etc\\schema-registry\\schema-registry.properties**\n",
    "\n",
    "\n",
    "5. Debezium MySQL CDC Connector<br>\n",
    "   5.1 Download:\n",
    "   - https://www.confluent.io/hub/debezium/debezium-connector-mysql\n",
    "   5.2 Create ***share\\java\\kafka\\plugins*** folder, and move downloaded jar files to the folder\n",
    "   - Ensure to check if there's latest debezium plugin version from official debezium website\n",
    "   5.3 Ensure ```plugin.path``` is set as ```plugin.path=share/java``` on ***etc\\kafka\\connect-distributed.properties*** file\n",
    "   5.4 Configure MySQL binlog:\n",
    "   - Reference:\n",
    "     - https://documentation.commvault.com/commvault/v11/article?p=34667.htm\n",
    "     - https://debezium.io/documentation/reference/0.9/connectors/mysql.html\n",
    "   - Comment out default ```log-bin``` value from ***my.ini*** file, and paste the following:\n",
    "   ```\n",
    "    log_bin=mysql-bin\n",
    "    binlog_format=row\n",
    "    binlog_row_image=full\n",
    "    expire_logs_days=1\n",
    "    gtid_mode=on\n",
    "    enforce_gtid_consistency=on\n",
    "    binlog_rows_query_log_events=on\n",
    "   ```\n",
    "   - Check MySQL variables:\n",
    "   ```\n",
    "   SHOW VARIABLES WHERE variable_name IN ('server_id','log_bin','binlog_format','binlog_row_image', 'expire_logs_days');\n",
    "   SHOW VARIABLES WHERE variable_name IN ('gtid_mode', 'enforce_gtid_consistency', 'binlog_rows_query_log_events');\n",
    "   ```\n",
    "\n",
    "\n",
    "6. Start Connector<br>\n",
    "   6.1 Execute one of the followings:<br>\n",
    "   - JSON Converter: ***bin\\windows\\connect-distributed.bat etc\\kafka\\connect-distributed.properties***\n",
    "   - Avro Converter: ***bin\\windows\\connect-distributed.bat etc\\schema-registry\\connect-avro-distributed.properties***\n",
    "   6.2 If encounter ```FileNotFoundException```:\n",
    "   - Open ***bin\\windows\\connect-distributed.bat***\n",
    "   - Search for ```rem Log4j settings``` in the bat file\n",
    "   - Replace ```config/tools-log4j.properties``` with ```etc/kafka/tools-log4j.properties```\n",
    "   - Re-run connector\n",
    "   6.3 Check if http://localhost:8083/ is running\n",
    "\n",
    "\n",
    "7. Start Debezium Source Connector (Producer)<br>\n",
    "   7.1 Run following on bash prompt, or perform POST request on Postman:\n",
    "   ```\n",
    "    curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors -d '{\n",
    "        \"name\": \"debezium-mysql-source-connector\",\n",
    "        \"config\": {\n",
    "            \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n",
    "            \"tasks.max\": 1,\n",
    "            \"database.hostname\": \"localhost\",\n",
    "            \"database.port\": \"3306\",\n",
    "            \"database.user\": \"root\",\n",
    "            \"database.password\": \"root\",\n",
    "            \"database.server.id\": \"184054\",\n",
    "            \"database.server.name\": \"test_server\",\n",
    "            \"database.whitelist\": \"testdb\",\n",
    "            \"table.whitelist\": \"testdb.test_table\",\n",
    "            \"database.history.kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "            \"database.history.kafka.topic\": \"TEST_TOPIC\",\n",
    "            \"snapshot.mode\": \"schema_only\"\n",
    "        }\n",
    "    }'\n",
    "   ```\n",
    "   7.2 If encounter unrecognized server timezone error:\n",
    "   - Download SQL script (POSIX standard) to populate timezone data at: https://dev.mysql.com/downloads/timezones.html\n",
    "   - Add ```USE mysql;``` at the beginning of script, and run it to populate timezone data\n",
    "   - Change timezone once complete: ```SET GLOBAL time_zone='Asia/Kuala_Lumpur'```\n",
    "   - Re-submit POST request\n",
    "   7.3 Ensure connector & tasks are in \"RUNNING\" state:\n",
    "   - http://localhost:8083/connectors/debezium-mysql-source-connector/status\n",
    "\n",
    "\n",
    "8. List topic (Optional)<br>\n",
    "   8.1 Execute ***bin\\windows\\kafka-topics.bat --list --bootstrap-server localhost:9092***<br>\n",
    "   8.2 <font color='red'>**NOTE**</font>: Topic naming convention to listen on will be: ```<serverName>.<databaseName>.<tableName>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroConsumer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from datetime import date, timedelta, datetime\n",
    "import struct\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/abrarsheikh/avro/blob/9407b60e03002e3e04ac31a81d54821377e051e3/lang/py/src/avro/io.py\n",
    "def date_from_int(days_since_epoc, return_format=None):\n",
    "    dt = date(1970, 1, 1) + timedelta(days_since_epoc)\n",
    "    return dt if return_format is None else dt.strftime(format=return_format)\n",
    "\n",
    "def timestamp_from_long(ts, return_format=None):\n",
    "    dt = datetime.utcfromtimestamp(ts / 1000.0)\n",
    "    return dt if return_format is None else dt.strftime(format=return_format)\n",
    "\n",
    "def decimal_from_byte(b):\n",
    "    req_length  = 8\n",
    "    diff_length = req_length - len(b)\n",
    "\n",
    "    return struct.unpack('!Q', (b'\\0' * diff_length) + b)[0] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_schema = avro.loads('''\n",
    "{\n",
    "   \"type\": \"record\",\n",
    "   \"name\": \"Envelope\",\n",
    "   \"namespace\": \"test_server.testdb.test_table\",\n",
    "   \"fields\": [\n",
    "      {\n",
    "         \"type\": [\n",
    "            \"null\",\n",
    "            {\n",
    "               \"type\": \"record\",\n",
    "               \"name\": \"Value\",\n",
    "               \"namespace\": \"test_server.testdb.test_table\",\n",
    "               \"fields\": [\n",
    "                  {\n",
    "                     \"type\": \"int\",\n",
    "                     \"name\": \"int_field\"\n",
    "                  },\n",
    "                  {\n",
    "                     \"type\": \"string\",\n",
    "                     \"name\": \"str_field\"\n",
    "                  },\n",
    "                  {\n",
    "                     \"type\": [\n",
    "                        \"null\",\n",
    "                        \"double\"\n",
    "                     ],\n",
    "                     \"name\": \"double_field\",\n",
    "                     \"default\": null\n",
    "                  },\n",
    "                  {\n",
    "                     \"type\": [\n",
    "                        \"null\",\n",
    "                        {\n",
    "                           \"type\": \"bytes\",\n",
    "                           \"logicalType\": \"decimal\"\n",
    "                        }\n",
    "                     ],\n",
    "                     \"name\": \"decimal_field\",\n",
    "                     \"default\": null\n",
    "                  },\n",
    "                  {\n",
    "                     \"type\": [\n",
    "                        \"null\",\n",
    "                        {\n",
    "                           \"type\": \"int\",\n",
    "                           \"logicalType\": \"date\"\n",
    "                        }\n",
    "                     ],\n",
    "                     \"name\": \"date_field\",\n",
    "                     \"default\": null\n",
    "                  },\n",
    "                  {\n",
    "                     \"type\": [\n",
    "                        \"null\",\n",
    "                        {\n",
    "                           \"type\": \"long\",\n",
    "                           \"logicalType\": \"timestamp-millis\"\n",
    "                        }\n",
    "                     ],\n",
    "                     \"name\": \"datetime_field\",\n",
    "                     \"default\": null\n",
    "                  }\n",
    "               ]\n",
    "            }\n",
    "         ],\n",
    "         \"name\": \"before\",\n",
    "         \"default\": null\n",
    "      },\n",
    "      {\n",
    "         \"type\": [\n",
    "            \"null\",\n",
    "            \"test_server.testdb.test_table.Value\"\n",
    "         ],\n",
    "         \"name\": \"after\",\n",
    "         \"default\": null\n",
    "      },\n",
    "      {\n",
    "         \"type\": \"string\",\n",
    "         \"name\": \"op\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = AvroConsumer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'group_id',\n",
    "    'schema.registry.url': 'http://127.0.0.1:8081',\n",
    "    'enable.auto.commit': 'False',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}, reader_value_schema=record_schema)\n",
    "c.subscribe(['test_server.testdb.test_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        message = c.poll(10)\n",
    "    except SerializerError as e:\n",
    "        print(f'[ERROR:SerializerError] - {e}')\n",
    "        try:\n",
    "            print(f'[FAILED MESSAGE]: {message}')\n",
    "        except NameError:\n",
    "            pass\n",
    "        break\n",
    "    \n",
    "    if message is None:\n",
    "        continue\n",
    "    \n",
    "    if message.error():\n",
    "        print(f'[ERROR:MessageError] - {message.error()}')\n",
    "        continue\n",
    "    \n",
    "    value = message.value()\n",
    "    for status in ['before', 'after']:\n",
    "        if value and value[status]:\n",
    "            value[status]['date_field']     = date_from_int(value[status]['date_field'], return_format='%Y-%m-%d')\n",
    "            value[status]['datetime_field'] = timestamp_from_long(value[status]['datetime_field'], return_format='%Y-%m-%d %H:%M:%S')\n",
    "            value[status]['decimal_field']  = decimal_from_byte(value[status]['decimal_field'])\n",
    "    \n",
    "    print(f'[CONSUMED]:')\n",
    "    print(json.dumps(value, indent=2))\n",
    "    print()\n",
    "\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest')\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consumer.subscribe(['test_server.testdb.test_table'])\n",
    "for message in consumer:\n",
    "    \n",
    "    value = json.loads('{}' if message.value is None else message.value)\n",
    "    value = {k: v for k,v in value.get('payload', {}).items() if k in ['before', 'after', 'op']}\n",
    "    \n",
    "    print(f'[CONSUMED]:')\n",
    "    print(json.dumps(value, indent=2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
