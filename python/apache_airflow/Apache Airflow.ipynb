{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to install Apache Airflow?\n",
    "- https://www.youtube.com/watch?v=SYOUbiGtGiU\n",
    "- https://coding-stream-of-consciousness.com/2018/11/06/apache-airflow-windows-10-install-ubuntu/\n",
    "\n",
    "How to start Apache Airflow?\n",
    "1. create airflow/dags folder on any directory\n",
    "2. open Ubuntu bash:\n",
    "   2.1 type 'cd airflow' to go to airflow directory\n",
    "   2.2 type 'ls' to view files\n",
    "   2.3 type 'nano airflow.cfg' to edit config file\n",
    "       >> change all directory to point to airflow directory created: /mnt/c/<your_workspace>/airflow\n",
    "       \n",
    "3. stop all airflow services:\n",
    "   3.1 type 'airflow kerberos -D'\n",
    "   3.2 type 'airflow scheduler -D'\n",
    "   3.3 type 'airflow webserver -D'\n",
    "   \n",
    "4. initialize and start airflow services:\n",
    "   4.1 type 'airflow initdb'\n",
    "   4.2 type 'airflow webserver -p 8080'\n",
    "   4.3 type 'airflow scheduler'\n",
    "   \n",
    "5. http://localhost:8080/admin\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "import pytz\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = airflow.utils.dates.days_ago(0)\n",
    "start_date = start_date.replace(tzinfo=pytz.utc).astimezone(pytz.timezone('Asia/Kuala_Lumpur'))\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': start_date,\n",
    "    'email': ['tan.lim@opcbiz.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create script1.py, script2.py and script3.py at 'D:\\_workspace\\airflow\\dags\\scripts' folder\n",
    "\n",
    "t1 = BashOperator(\n",
    "    task_id='run_script1',\n",
    "    bash_command='python /mnt/d/_workspace/airflow/dags/scripts/script1.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='run_script2',\n",
    "    bash_command='python /mnt/d/_workspace/airflow/dags/scripts/script2.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='run_script3',\n",
    "    bash_command='python /mnt/d/_workspace/airflow/dags/scripts/script3.py',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "    'Sequential_Airflow_Tutorial',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='*/1 * * * *',\n",
    "    catchup=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 runs first, t2 runs second, followed by t3\n",
    "t1 >> t2 >> t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "    'Parallel_Airflow_Tutorial',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='*/1 * * * *',\n",
    "    catchup=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# - required to change 'executor = LocalExecutor' at airflow.cfg file\n",
    "# - required to change 'sql_alchemy_conn' to use actual database\n",
    "\n",
    "# t1 runs first, and t2, t3 runs parallely\n",
    "t1 >> t2\n",
    "t1 >> t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
